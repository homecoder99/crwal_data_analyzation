"""
ë©”ì¸ í¬ë¡¤ë§ ì‹¤í–‰ê¸°
Excel ì²˜ë¦¬ì™€ í¬ë¡¤ë§ì„ í†µí•©í•˜ì—¬ tqdm ì§„í–‰ë¥  í‘œì‹œ
"""
import asyncio
import logging
import time
from tqdm.asyncio import tqdm
from excel_processor import ExcelProcessor
from crawler import OliveYoungCrawler

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class MainCrawler:
    def __init__(self, 
                 excel_path: str = "data/Qoo10_ItemInfo.xlsx",
                 max_concurrent: int = 3,
                 delay_range: tuple = (1, 3),
                 output_file: str = "olive_young_products.json"):
        """
        ë©”ì¸ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            excel_path: Excel íŒŒì¼ ê²½ë¡œ
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
            delay_range: ìš”ì²­ ê°„ ë”œë ˆì´ ë²”ìœ„ (ì´ˆ)
            output_file: ê²°ê³¼ ì €ì¥ íŒŒì¼ëª…
        """
        self.excel_path = excel_path
        self.max_concurrent = max_concurrent
        self.delay_range = delay_range
        self.output_file = output_file
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.excel_processor = ExcelProcessor(excel_path)
        self.crawler = OliveYoungCrawler(
            max_concurrent=max_concurrent,
            delay_range=delay_range,
            output_file=output_file
        )
    
    async def run_with_progress(self):
        """ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("ğŸ¯ ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ í¬ë¡¤ë§ ì‹œì‘")
        start_time = time.time()
        
        # Step 1: Excel ë°ì´í„° ì²˜ë¦¬
        logger.info("ğŸ“‹ Step 1: Excel ë°ì´í„° ì²˜ë¦¬")
        product_ids = self.excel_processor.process()
        
        if not product_ids:
            logger.error("âŒ Excelì—ì„œ ìœ íš¨í•œ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None
        
        total_ids = len(product_ids)
        logger.info(f"âœ… ì²˜ë¦¬í•  ìƒí’ˆ ID: {total_ids}ê°œ")
        
        # Step 2: í¬ë¡¤ë§ ì„¤ì •
        logger.info("ğŸ”§ Step 2: í¬ë¡¤ë§ ì„¤ì •")
        estimated_time = total_ids * sum(self.delay_range) / 2 / self.max_concurrent
        logger.info(f"â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: {estimated_time/60:.1f}ë¶„")
        
        # Step 3: í¬ë¡¤ë§ ì‹¤í–‰ (tqdm ì§„í–‰ë¥  í‘œì‹œ)
        logger.info("ğŸš€ Step 3: í¬ë¡¤ë§ ì‹¤í–‰")
        
        # tqdmì„ ìœ„í•œ ì§„í–‰ë¥  ì¶”ì  ë³€ìˆ˜
        progress_bar = tqdm(
            total=total_ids,
            desc="í¬ë¡¤ë§ ì§„í–‰",
            unit="ìƒí’ˆ",
            colour="green",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"
        )
        
        # í¬ë¡¤ëŸ¬ í†µê³„ ì¶”ì ì„ ìœ„í•œ ì½œë°± ì„¤ì •
        original_check_product_availability = self.crawler.check_product_availability
        
        async def check_product_availability_with_progress(context, product_id):
            result = await original_check_product_availability(context, product_id)
            progress_bar.update(1)  # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            
            # ì‹¤ì‹œê°„ í†µê³„ ì—…ë°ì´íŠ¸
            stats = self.crawler.stats
            progress_bar.set_postfix({
                'ì„±ê³µ': stats['success'],
                'ì‹¤íŒ¨': stats['failed']
            })
            
            return result
        
        # í¬ë¡¤ëŸ¬ ë©”ì„œë“œ ì˜¤ë²„ë¼ì´ë“œ
        self.crawler.check_product_availability = check_product_availability_with_progress
        
        try:
            # í¬ë¡¤ë§ ì‹¤í–‰
            results = await self.crawler.crawl_products(product_ids)
            
        finally:
            progress_bar.close()
        
        # Step 4: ê²°ê³¼ ìš”ì•½
        total_time = time.time() - start_time
        
        logger.info("=" * 80)
        logger.info("ğŸ† í¬ë¡¤ë§ ì‘ì—… ì™„ë£Œ!")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_time/60:.2f}ë¶„ ({total_time:.1f}ì´ˆ)")
        logger.info(f"ğŸ“ˆ ì²˜ë¦¬ ì†ë„: {total_ids/total_time:.2f} ìƒí’ˆ/ì´ˆ")
        logger.info(f"âœ… ì„±ê³µ: {results['stats']['success']}ê°œ")
        logger.info(f"âŒ ì‹¤íŒ¨: {results['stats']['failed']}ê°œ")
        logger.info(f"ğŸ“Š ì„±ê³µë¥ : {results['stats']['success']/total_ids*100:.1f}%")
        logger.info(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {self.output_file}")
        logger.info("=" * 80)
        
        return results
    
    def run(self):
        """ë™ê¸° ì‹¤í–‰ wrapper"""
        return asyncio.run(self.run_with_progress())

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”¥ ì˜¬ë¦¬ë¸Œì˜ ìƒí’ˆ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 80)
    
    # ì„¤ì •
    config = {
        'excel_path': 'data/Qoo10_ItemInfo.xlsx',
        'max_concurrent': 5,  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì¦ê°€ (3 -> 5)
        'delay_range': (0.5, 1.5),  # ìš”ì²­ ê°„ ë”œë ˆì´ ê°ì†Œ (2-4ì´ˆ -> 0.5-1.5ì´ˆ)
        'output_file': 'olive_young_products.json'
    }
    
    print("âš™ï¸  í¬ë¡¤ë§ ì„¤ì •:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print("=" * 80)
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    main_crawler = MainCrawler(**config)
    
    try:
        results = main_crawler.run()
        
        if results:
            print("\nğŸ‰ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ğŸ“„ ê²°ê³¼ëŠ” '{config['output_file']}'ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâŒ í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nğŸ’¥ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        logger.error(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()